<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jon Barron</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jon Barron</name>
              </p>
              <p>I am a senior staff research scientist at <a href="https://ai.google/research">Google Research</a>, where I work on computer vision and machine learning.
              </p>
              <p>
                At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">Jump</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, and <a href="https://www.matthewtancik.com/nerf">NeRF</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I've received the <a href="https://www2.eecs.berkeley.edu/Students/Awards/15/">C.V. Ramamoorthy Distinguished Research Award</a> and the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:jonbarron@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/jonbarron/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/JonBarron.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/JonBarron_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, machine learning, optimization, and image processing. Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images. Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					

          <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dreamfusion_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/dreamfusion.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/dreamfusion.jpg' width="160">
              </div>
              <script type="text/javascript">
                function dreamfusion_start() {
                  document.getElementById('dreamfusion_image').style.opacity = "1";
                }

                function dreamfusion_stop() {
                  document.getElementById('dreamfusion_image').style.opacity = "0";
                }
                dreamfusion_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://dreamfusion3d.github.io/">
                <papertitle>DreamFusion: Text-to-3D using 2D Diffusion</papertitle>
              </a>
              <br>
              <a href="https://cs.stanford.edu/~poole/">Ben Poole</a>,
              <a href="https://www.ajayj.com/">Ajay Jain</a>,
              <strong>Jonathan T. Barron</strong>,
							<a href="https://bmild.github.io/">Ben Mildenhall</a>
              <br>
              <em>arXiv</em>, 2022
              <br>
              <a href="https://dreamfusion3d.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
              /
              <a href="https://dreamfusion3d.github.io/gallery.html">gallery</a>
              <p></p>
              <p>
              We optimize a NeRF from scratch using a pretrained text-to-image diffusion model to do text-to-3D generative modeling.
              </p>
            </td>
          </tr>
		  
          <tr onmouseout="samurai_stop()" onmouseover="samurai_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='samurai_image'>
                  <img src='images/samurai_after.jpg' width="160"></div>
                <img src='images/samurai_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function samurai_start() {
                  document.getElementById('samurai_image').style.opacity = "1";
                }

                function samurai_stop() {
                  document.getElementById('samurai_image').style.opacity = "0";
                }
                samurai_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://markboss.me/publication/2022-samurai/">
                <papertitle>SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image Collections</papertitle>
              </a>
              <br>
              <a href="https://markboss.me">Mark Boss</a>, 
              <a href="">Andreas Engelhardt</a>, 
              <a href="https://abhishekkar.info/">Abhishek Kar</a>, 
              <a href="http://people.csail.mit.edu/yzli/">Yuanzhen Li</a>, 
              <a href="https://deqings.github.io/">Deqing Sun</a>, 
              <strong>Jonathan T. Barron</strong>,
              <a href="https://uni-tuebingen.de/en/faculties/faculty-of-science/departments/computer-science/lehrstuehle/computergrafik/computer-graphics/staff/prof-dr-ing-hendrik-lensch/">Hendrik P. A. Lensch</a>,
              <a href="https://varunjampani.github.io">Varun Jampani</a>
              <br>
              <em>NeurIPS</em>, 2022
              <br>
              <a href="https://markboss.me/publication/2022-samurai/">project page</a> /
              <a href="https://www.youtube.com/watch?v=LlYuGDjXp-8">video</a> /
              <a href="https://arxiv.org/abs/2205.15768">arXiv</a>
              <p></p>
              <p>
A joint optimization framework for estimating shape, BRDF, camera pose, and illumination from in-the-wild image collections.
              </p>
            </td>
          </tr>		

          <tr onmouseout="pnf_stop()" onmouseover="pnf_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
          <div class="two" id='pnf_image'>
            <img src='images/pnf_before.jpg' width="160"></div>
          <img src='images/pnf_after.jpg' width="160">
          </div>
          <script type="text/javascript">
          function pnf_start() {
            document.getElementById('pnf_image').style.opacity = "1";
          }

          function pnf_stop() {
            document.getElementById('pnf_image').style.opacity = "0";
          }
          pnf_stop()
          </script>
          </td>
          <td style="padding:0;width:40%;max-width:40%">
          <a href="TODO">
          <papertitle>Polynomial Neural Fields for Subband Decomposition</papertitle>
          </a> <br>
          <a href="https://www.guandaoyang.com/">Guandao Yang*</a>,
          <a href="https://sagiebenaim.github.io/">Sagie Benaim*</a>,
          <a href="https://varunjampani.github.io/">Varun Jampani</a>,
          <a href="https://www.kylegenova.com/">Kyle Genova</a>,
          <strong>Jonathan T. Barron</strong>,
          <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>,
          <a href="http://home.bharathh.info/">Bharath Hariharan</a>,
          <a href="https://sergebelongie.github.io/">Serge Belongie</a>
          <br>
          <em>NeurIPS</em>, 2022
          <p>
          Representing neural fields as a composition of manipulable and interpretable components lets you do things like reason about frequencies and scale.
          </p>
          </td>
          </tr> 


          <tr onmouseout="malle_stop()" onmouseover="malle_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/MalleConv_after.jpg' width="160"></div>
                <img src='images/MalleConv_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://yifanjiang.net/MalleConv.html">
                <papertitle>Fast and High-quality Image Denoising via Malleable Convolutions</papertitle>
              </a>
              <br>
              <a href="https://yifanjiang.net/">Yifan Jiang</a>,
              <a href="https://bartwronski.com/">Bartlomiej Wronski</a>, 
							<a href="https://bmild.github.io/">Ben Mildenhall</a>, <br>
              <strong>Jonathan T. Barron</strong>,
              <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Zhangyang Wang</a>,
              <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://yifanjiang.net/MalleConv.html">project page</a>
              /
              <a href="https://arxiv.org/abs/2201.00392">arXiv</a>
              <p></p>
              <p>
              We denoise images efficiently by predicting spatially-varying kernels at low resolution and using a fast fused op to jointly upsample and apply these kernels at full resolution.
              </p>
            </td>
          </tr>
					
          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/nerf_supervision.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerf_supervision.jpg' width="160">
              </div>
              <script type="text/javascript">
                function nerfsuper_start() {
                  document.getElementById('nerfsuper_image').style.opacity = "1";
                }

                function nerfsuper_stop() {
                  document.getElementById('nerfsuper_image').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="http://yenchenlin.me/nerf-supervision/">
                <papertitle>NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields</papertitle>
              </a>
              <br>
              <a href="https://yenchenlin.me/">Lin Yen-Chen</a>, 
              <a href="http://www.peteflorence.com/">Pete Florence</a>, 
              <strong>Jonathan T. Barron</strong>,  <br>
              <a href="https://scholar.google.com/citations?user=_BPdgV0AAAAJ&hl=en">Tsung-Yi Lin</a>, 
              <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,
              <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>
              <br>
              <em>ICRA</em>, 2022  
              <br>
							<a href="http://yenchenlin.me/nerf-supervision/">project page</a> / 
							<a href="https://arxiv.org/abs/2203.01913">arXiv</a> / 
							<a href="https://www.youtube.com/watch?v=_zN-wVwPH1s">video</a> /
							<a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> / 
							<a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				
              <p></p>
              <p>NeRF works better than RGB-D cameras or multi-view stereo when learning object descriptors.</p>
            </td>
          </tr>

			    <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()"  bgcolor="#ffffd0">
			      <td style="padding:20px;width:25%;vertical-align:middle">
			        <div class="one">
			          <div class="two" id='refnerf_image'><video  width=100% height=100% muted autoplay loop>
			          <source src="images/refnerf.mp4" type="video/mp4">
			          Your browser does not support the video tag.
			          </video></div>
			          <img src='images/refnerf.jpg' width="160">
			        </div>
			        <script type="text/javascript">
			          function refnerf_start() {
			            document.getElementById('refnerf_image').style.opacity = "1";
			          }

			          function refnerf_stop() {
			            document.getElementById('refnerf_image').style.opacity = "0";
			          }
			          refnerf_stop()
			        </script>
			      </td>
			            <td style="padding:20px;width:75%;vertical-align:middle">
			          <a href="https://dorverbin.github.io/refnerf/index.html">
			            <papertitle>Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields</papertitle>
			          </a>
			          <br>
			          <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
			          <a href="https://phogzone.com/">Peter Hedman</a>,
			          <a href="https://bmild.github.io/">Ben Mildenhall</a>, <br>
			          <a href="Todd Zickler">Todd Zickler</a>,
			          <strong>Jonathan T. Barron</strong>,
			          <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
			          <br>
			    <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation, Best Student Paper Honorable Mention)</strong></font>
			          <br>
			          <a href="https://dorverbin.github.io/refnerf/index.html">project page</a>
			    /
			          <a href="https://arxiv.org/abs/2112.03907">arXiv</a>
			    /
			          <a href="https://youtu.be/qrdRH9irAlk">video</a>
			          <p></p>
			          <p>Explicitly modeling reflections in NeRF produces realistic shiny surfaces and accurate surface normals, and lets you edit materials.</p>
			        </td>
			      </tr>
						
          <tr onmouseout="mip360_stop()" onmouseover="mip360_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mip360_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/mip360_sat.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/mip360_sat.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('mip360_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('mip360_image').style.opacity = "0";
                }
                mip360_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://jonbarron.info/mipnerf360">
                <papertitle>Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
              <a href="https://phogzone.com/">Peter Hedman</a>
              <br>
							<em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="http://jonbarron.info/mipnerf360">project page</a>
              /
              <a href="https://arxiv.org/abs/2111.12077">arXiv</a>
              /
              <a href="https://youtu.be/zBSH-k9GbV4">video</a>
              <p></p>
              <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p>
            </td>
          </tr> 

          <tr onmouseout="rawnerf_stop()" onmouseover="rawnerf_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='rawnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/rawnerf.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/rawnerf.jpg' width="160">
              </div>
              <script type="text/javascript">
                function rawnerf_start() {
                  document.getElementById('rawnerf_image').style.opacity = "1";
                }

                function rawnerf_stop() {
                  document.getElementById('rawnerf_image').style.opacity = "0";
                }
                rawnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://bmild.github.io/rawnerf/index.html">
                <papertitle>NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images</papertitle>
              </a>
              <br>
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="https://phogzone.com/">Peter Hedman</a>,
              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>, <br>
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
              <strong>Jonathan T. Barron</strong>
              <br>
							<em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://bmild.github.io/rawnerf/index.html">project page</a>
        /
              <a href="https://arxiv.org/abs/2111.13679">arXiv</a>
        /
              <a href="https://www.youtube.com/watch?v=JtBS4KBcKVc">video</a>
              <p></p>
              <p>
								Properly training NeRF on raw camera data enables HDR view synthesis and bokeh, and outperforms multi-image denoising.</p>
            </td>
          </tr> 
					
   
          <tr onmouseout="regnerf_stop()" onmouseover="regnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='regnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/regnerf_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/regnerf_before.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function regnerf_start() {
                  document.getElementById('regnerf_image').style.opacity = "1";
                }

                function regnerf_stop() {
                  document.getElementById('regnerf_image').style.opacity = "0";
                }
                regnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://m-niemeyer.github.io/regnerf/index.html">
                <papertitle>RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs</papertitle>
              </a>
              <br>
              <a href="https://m-niemeyer.github.io/">Michael Niemeyer</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>, <br>
              <a href="https://msmsajjadi.github.io/">Mehdi S. M. Sajjadi</a>, 
              <a href="http://www.cvlibs.net/">Andreas Geiger</a>,
              <a href="http://www2.informatik.uni-freiburg.de/~radwann/">Noha Radwan</a>
              <br>
        <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://m-niemeyer.github.io/regnerf/index.html">project page</a>
        /
              <a href="https://arxiv.org/abs/2112.00724">arXiv</a>
        /
              <a href="https://www.youtube.com/watch?v=QyyyvA4-Kwc">video</a>
              <p></p>
              <p>Regularizing unseen views during optimization enables view synthesis from as few as 3 input images.</p>
            </td>
          </tr> 


          <tr onmouseout="blocknerf_stop()" onmouseover="blocknerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='blocknerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/blocknerf_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/blocknerf_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function blocknerf_start() {
                  document.getElementById('blocknerf_image').style.opacity = "1";
                }

                function blocknerf_stop() {
                  document.getElementById('blocknerf_image').style.opacity = "0";
                }
                blocknerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://waymo.com/research/block-nerf/">
                <papertitle>Block-NeRF: Scalable Large Scene Neural View Synthesis</papertitle>
              </a>
              <br>
              <a href="http://matthewtancik.com/">Matthew Tancik</a>,
              <a href="http://casser.io/">Vincent Casser</a>,
              <a href="https://sites.google.com/site/skywalkeryxc/">Xinchen Yan</a>,
              <a href="https://scholar.google.com/citations?user=5mJUkI4AAAAJ&hl=en">Sabeek Pradhan</a>, <br>
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
							<a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="https://www.henrikkretzschmar.com/">Henrik Kretzschmar</a>
              <br>
        <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://waymo.com/research/block-nerf/">project page</a>
        /
              <a href="https://arxiv.org/abs/2202.05263">arXiv</a>
        /
              <a href="https://www.youtube.com/watch?v=6lGMCAzBzOQ">video</a>
              <p></p>
              <p>We can do city-scale reconstruction by training multiple NeRFs with millions of images.</p>
            </td>
          </tr>
					
          <tr onmouseout="hnerf_stop()" onmouseover="hnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/hnerf_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/hnerf_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function hnerf_start() {
                  document.getElementById('hnerf_image').style.opacity = "1";
                }

                function hnerf_stop() {
                  document.getElementById('hnerf_image').style.opacity = "0";
                }
                hnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://grail.cs.washington.edu/projects/humannerf/">
                <papertitle>HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video</papertitle>
              </a>
              <br>
              <a href="https://homes.cs.washington.edu/~chungyi/">Chung-Yi Weng</a>,
              <a href="https://homes.cs.washington.edu/~curless/">Brian Curless</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>, <br>
              <strong>Jonathan T. Barron</strong>,
              <a href="https://www.irakemelmacher.com/">Ira Kemelmacher-Shlizerman </a>
              <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://grail.cs.washington.edu/projects/humannerf/">project page</a>
              /
              <a href="https://arxiv.org/abs/2201.04127">arXiv</a>
              /
              <a href="https://youtu.be/GM-RoZEymmw">video</a>
              <p></p>
              <p>Combining NeRF with pose estimation lets you use a monocular video to do free-viewpoint rendering of a human.</p>
            </td>
          </tr>
					
          <tr onmouseout="urf_stop()" onmouseover="urf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='urf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/urf.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/urf.jpg' width="160">
              </div>
              <script type="text/javascript">
                function urf_start() {
                  document.getElementById('urf_image').style.opacity = "1";
                }

                function urf_stop() {
                  document.getElementById('urf_image').style.opacity = "0";
                }
                urf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://urban-radiance-fields.github.io/">
                <papertitle>Urban Radiance Fields</papertitle>
              </a>
              <br>
							<a href="http://www.krematas.com/">Konstantinos Rematas</a>,
							<a href="https://andrewhliu.github.io/">Andrew Liu</a>,
							<a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>,
							<strong>Jonathan T. Barron</strong>, <br>
							<a href="https://taiya.github.io/">Andrea Tagliasacchi</a>,
							<a href="https://www.cs.princeton.edu/~funk/">Tom Funkhouser</a>,
							<a href="https://sites.google.com/corp/view/vittoferrari"> Vittorio Ferrari</a>
              <br>
							<em>CVPR</em>, 2022
              <br>
              <a href="https://urban-radiance-fields.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2111.14643">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=qGlq5DZT6uc">video</a>
              <p></p>
              <p>
								Incorporating lidar and explicitly modeling the sky lets you reconstruct urban environments.</p>
            </td>
          </tr> 

	
  <tr onmouseout="ddp_stop()" onmouseover="ddp_start()">
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <div class="two" id='ddp_image'>
          <img src='images/ddp_after.jpg' width="160"></div>
        <img src='images/ddp_before.jpg' width="160">
      </div>
      <script type="text/javascript">
        function ddp_start() {
          document.getElementById('ddp_image').style.opacity = "1";
        }

        function ddp_stop() {
          document.getElementById('ddp_image').style.opacity = "0";
        }
        ddp_stop()
      </script>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://arxiv.org/abs/2112.03288">
        <papertitle>Dense Depth Priors for Neural Radiance Fields from Sparse Input Views</papertitle>
      </a>
      <br>
			<a href="https://niessnerlab.org/members/barbara_roessle/profile.html">Barbara Roessle</a>,
			<strong>Jonathan T. Barron</strong>,
			<a href="https://bmild.github.io/">Ben Mildenhall</a>, 
			<a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>, 
			<a href="https://www.niessnerlab.org/">Matthias Nie√üner</a>
      <br>
			<em>CVPR</em>, 2022
      <br>
      <a href="https://arxiv.org/abs/2112.03288">arXiv</a>
      /
      <a href="https://www.youtube.com/watch?v=zzkvvdcvksc">video</a>
      <p></p>
      <p>
      Dense depth completion techniques applied to freely-available sparse stereo data can improve NeRF reconstructions in low-data regimes.
      </p>
    </td>
  </tr>
	
          <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='clipnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/dreamfield_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/dreamfield_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function clipnerf_start() {
                  document.getElementById('clipnerf_image').style.opacity = "1";
                }

                function clipnerf_stop() {
                  document.getElementById('clipnerf_image').style.opacity = "0";
                }
                clipnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ajayj.com/dreamfields">
                <papertitle>Zero-Shot Text-Guided Object Generation with Dream Fields</papertitle>
              </a>
              <br>
              <a href="https://www.ajayj.com/">Ajay Jain</a>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
              <a href="https://cs.stanford.edu/~poole/">Ben Poole</a>
              <br>
        <em>CVPR</em>, 2022
              <br>
              <a href="https://ajayj.com/dreamfields">project page</a>
        /
              <a href="https://arxiv.org/abs/2112.01455">arXiv</a>
        /
              <a href="https://www.youtube.com/watch?v=1Fke6w46tv4">video</a>
              <p></p>
              <p>Supervising the CLIP embeddings of NeRF renderings lets you to generate 3D objects from text prompts.</p>
            </td>
          </tr> 
    
          <tr onmouseout="survey_stop()" onmouseover="survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='survey_image'>
                  <img src='images/survey_after.png' width="160"></div>
                <img src='images/survey_before.png' width="160">
              </div>
              <script type="text/javascript">
                function survey_start() {
                  document.getElementById('survey_image').style.opacity = "1";
                }

                function survey_stop() {
                  document.getElementById('survey_image').style.opacity = "0";
                }
                survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2111.05849">
                <papertitle>Advances in Neural Rendering</papertitle>
              </a>
              <br>
							<a href="https://people.mpi-inf.mpg.de/~atewari/">Ayush Tewari</a>, 
							<a href="https://justusthies.github.io/">Justus Thies</a>, 
							<a href="https://bmild.github.io/">Ben Mildenhall</a>, 
							<a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>, 
							<a href="https://people.mpi-inf.mpg.de/~tretschk/">Edgar Tretschk</a>,
							<a href="https://homes.cs.washington.edu/~yifan1/">Yifan Wang</a>,
							<a href="https://christophlassner.de/">Christoph Lassner</a>,
							<a href="https://vsitzmann.github.io/">Vincent Sitzmann</a>,
							<a href="http://ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
							<a href="https://stephenlombardi.github.io/">Stephen Lombardi</a>,
							<a href="http://www.cs.cmu.edu/~tsimon/">Tomas Simon</a>,
							<a href="https://www.mpi-inf.mpg.de/departments/visual-computing-and-artificial-intelligence">Christian Theobalt</a>,
							<a href="https://www.niessnerlab.org/">Matthias Niessner</a>,
							<strong>Jonathan T. Barron</strong>,
							<a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>,
							<a href="https://zollhoefer.com/">Michael Zollhoefer</a>,
							<a href="https://people.mpi-inf.mpg.de/~golyanik/">Vladislav Golyanik</a>
              <br>
							<em>Arxiv</em>, 2021
              <br>
              <p></p>
              <p>
              A survey of recent progress in neural rendering.
              </p>
            </td>
          </tr>
					
          <tr onmouseout="npil_stop()" onmouseover="npil_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil_image'>
                  <img src='images/npil_after.jpg' width="160"></div>
                <img src='images/npil_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function npil_start() {
                  document.getElementById('npil_image').style.opacity = "1";
                }

                function npil_stop() {
                  document.getElementById('npil_image').style.opacity = "0";
                }
                npil_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://markboss.me/publication/2021-neural-pil/">
                <papertitle>Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition</papertitle>
              </a>
              <br>

              <a href="https://markboss.me">Mark Boss</a>, 
              <a href="https://varunjampani.github.io">Varun Jampani</a>,
              <a href="https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/raphael-braun/">Raphael Braun</a>, <br>
              <a href="http://people.csail.mit.edu/celiu/">Ce Liu</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="https://uni-tuebingen.de/en/faculties/faculty-of-science/departments/computer-science/lehrstuehle/computergrafik/computer-graphics/staff/prof-dr-ing-hendrik-lensch/">Hendrik P. A. Lensch</a>
              <br>
							<em>NeurIPS</em>, 2021
              <br>
              <a href="https://markboss.me/publication/2021-neural-pil/">project page</a> /
              <a href="https://www.youtube.com/watch?v=p5cKaNwVp4M">video</a> /
              <a href="https://arxiv.org/abs/2110.14373">arXiv</a>
              <p></p>
              <p>
              Replacing a costly illumination integral with a simple network query enables more accurate novel view-synthesis and relighting compared to NeRD.
              </p>
            </td>
          </tr>
					
					
          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hypernerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/hypernerf_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/hypernerf_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function hypernerf_start() {
                  document.getElementById('hypernerf_image').style.opacity = "1";
                }

                function hypernerf_stop() {
                  document.getElementById('hypernerf_image').style.opacity = "0";
                }
                hypernerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://hypernerf.github.io/">
                <papertitle>HyperNeRF: A Higher-Dimensional Representation
for Topologically Varying Neural Radiance Fields</papertitle>
              </a>
              <br>
							<a href="https://keunhong.com">Keunhong Park</a>,
							<a href="https://utkarshsinha.com">Utkarsh Sinha</a>, 
							<a href="https://phogzone.com/">Peter Hedman</a>,
              <strong>Jonathan T. Barron</strong>, <br>
							<a href="http://sofienbouaziz.com">Sofien Bouaziz</a>,
							<a href="https://www.danbgoldman.com">Dan B Goldman</a>,
							<a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a>, 
							<a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a>
              <br>
              <em>SIGGRAPH Asia</em>, 2021 
              <br>
              <a href="https://hypernerf.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2106.13228">arXiv</a>
              <p></p>
              <p>Applying ideas from level set methods to NeRF lets you represent scenes that deform and change shape.</p>
            </td>
          </tr> 

          <tr onmouseout="nerfactor_stop()" onmouseover="nerfactor_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfactor_image'>
                  <img src='images/nerfactor_after.png' width="160"></div>
                <img src='images/nerfactor_before.png' width="160">
              </div>
              <script type="text/javascript">
                function nerfactor_start() {
                  document.getElementById('nerfactor_image').style.opacity = "1";
                }

                function nerfactor_stop() {
                  document.getElementById('nerfactor_image').style.opacity = "0";
                }
                nerfactor_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://people.csail.mit.edu/xiuming/projects/nerfactor/">
              <papertitle>NeRFactor: Neural Factorization of Shape and Reflectance<br>
Under an Unknown Illumination</papertitle>
              </a>
              <br>
              <a href="https://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
              <a href="https://boyangdeng.com/">Boyang Deng</a>,<br>
              <a href="https://www.pauldebevec.com/">Paul Debevec</a>,
              <a href="http://billf.mit.edu/">William T. Freeman</a>,
							<strong>Jonathan T. Barron</strong>
              <br>
              <em>SIGGRAPH Asia</em>, 2021 
              <br>
              <a href="https://people.csail.mit.edu/xiuming/projects/nerfactor/">project page</a>
              /
              <a href="https://arxiv.org/abs/2106.01970">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=UUVSPJlwhPg">video</a>
              <p></p>
              <p>By placing priors on illumination and materials, we can recover NeRF-like models of the intrinsics of a scene from a single multi-image capture.</p>
            </td>
						
          <tr onmouseout="dualfont_stop()" onmouseover="dualfont_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dualfont_image'><img src='images/dualfont_after.png'></div>
                <img src='images/dualfont_before.png'>
              </div>
              <script type="text/javascript">
                function dualfont_start() {
                  document.getElementById('dualfont_image').style.opacity = "1";
                }

                function dualfont_stop() {
                  document.getElementById('dualfont_image').style.opacity = "0";
                }
                dualfont_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2109.06627">
                <papertitle>Scalable Font Reconstruction with Dual Latent Manifolds</papertitle>
              </a>
              <br>
              <a href="http://www.cs.cmu.edu/~asrivats/">Nikita Srivatsan</a>,
              <a href="http://siwu.io/">Si Wu</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="http://cseweb.ucsd.edu/~tberg/">Taylor Berg-Kirkpatrick</a>
              <br>
              <em>EMNLP</em>, 2021
              <br>
              <p></p>
              <p>VAEs can be used to disentangle a font's style from its content, and to generalize to characters that were never observed during training.</p>
            </td>
          </tr>
					
          <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mipnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/mipnerf_ipe_yellow.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/mipnerf_ipe_yellow.png' width="160">
              </div>
              <script type="text/javascript">
                function mipnerf_start() {
                  document.getElementById('mipnerf_image').style.opacity = "1";
                }

                function mipnerf_stop() {
                  document.getElementById('mipnerf_image').style.opacity = "0";
                }
                mipnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://jonbarron.info/mipnerf">
                <papertitle>Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="http://matthewtancik.com/">Matthew Tancik</a>, <br>
              <a href="https://phogzone.com/">Peter Hedman</a>,
              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
              <br>
							<em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Honorable Mention)</strong></font>
              <br>
              <a href="http://jonbarron.info/mipnerf">project page</a>
              /
              <a href="https://arxiv.org/abs/2103.13415">arXiv</a>
              /
              <a href="https://youtu.be/EpH175PY1A0">video</a>
							/
              <a href="https://github.com/google/mipnerf">code</a>
              <p></p>
              <p>NeRF is aliased, but we can anti-alias it by casting cones and prefiltering the positional encoding function.</p>
            </td>
          </tr> 

          <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfbake_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/nerfbake_15.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerfbake_160.png' width="160">
              </div>
              <script type="text/javascript">
                function nerfbake_start() {
                  document.getElementById('nerfbake_image').style.opacity = "1";
                }

                function nerfbake_stop() {
                  document.getElementById('nerfbake_image').style.opacity = "0";
                }
                nerfbake_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://nerf.live">
              <papertitle>Baking Neural Radiance Fields for Real-Time View Synthesis</papertitle>
              </a>
              <br>
              <a href="https://phogzone.com/">Peter Hedman</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="https://www.pauldebevec.com/">Paul Debevec</a>
              <br>
							<em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="http://nerf.live">project page</a>
              /
              <a href="https://arxiv.org/abs/2103.14645">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=5jKry8n5YO8">video</a>
              /
              <a href="https://nerf.live/#demos">demo</a>
              <p></p>
              <p>Baking a trained NeRF into a sparse voxel grid of colors and features lets you render it in real-time in your browser.</p>
            </td>



          <tr onmouseout="nerfie_stop()" onmouseover="nerfie_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfie_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/nerfie_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerfie_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function nerfie_start() {
                  document.getElementById('nerfie_image').style.opacity = "1";
                }
                function nerfie_stop() {
                  document.getElementById('nerfie_image').style.opacity = "0";
                }
                nerfie_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://nerfies.github.io/">
                <papertitle>Nerfies: Deformable Neural Radiance Fields</papertitle>
              </a>
              <br>
              
              <a href="https://keunhong.com">Keunhong Park</a>,
              <a href="https://utkarshsinha.com">Utkarsh Sinha</a>,
              <strong>Jonathan T. Barron</strong>, <br>
              <a href="http://sofienbouaziz.com">Sofien Bouaziz</a>,
              <a href="https://www.danbgoldman.com">Dan B Goldman</a>,
              <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a>,
              <a href="http://www.ricardomartinbrualla.com">Ricardo-Martin Brualla</a>
              <br>
							<em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://nerfies.github.io/">project page</a> /
              <a href="https://arxiv.org/abs/2011.12948">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA">video</a>
              <p></p>
              <p>Building deformation fields into NeRF lets you capture non-rigid subjects, like people.
              </p>
            </td>
          </tr> 


          <tr onmouseout="c5_stop()" onmouseover="c5_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='c5_image'>
                  <img src='images/c5_after.jpg' width="160"></div>
                <img src='images/c5_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function c5_start() {
                  document.getElementById('c5_image').style.opacity = "1";
                }

                function c5_stop() {
                  document.getElementById('c5_image').style.opacity = "0";
                }
                c5_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2011.11890">
                <papertitle>Cross-Camera Convolutional Color Constancy</papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/corp/view/mafifi">Mahmoud Afifi</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="http://www.chloelegendre.com/">Chloe LeGendre</a>,
              <a href="https://research.google/people/105312/">Yun-Ta Tsai</a>,
              <a href="https://www.linkedin.com/in/fbleibel/">Francois Bleibel</a>
              <br>
							<em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <p></p>
              <p>
                With some extra (unlabeled) test-set images, you can build a hypernetwork that calibrates itself at test time to previously-unseen cameras.
              </p>
            </td>
          </tr> 


          <tr onmouseout="dualdefocus_stop()" onmouseover="dualdefocus_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dualdefocus_image'>
                  <img src='images/dualdefocus_after.jpg' width="160"></div>
                <img src='images/dualdefocus_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function dualdefocus_start() {
                  document.getElementById('dualdefocus_image').style.opacity = "1";
                }

                function dualdefocus_stop() {
                  document.getElementById('dualdefocus_image').style.opacity = "0";
                }
                dualdefocus_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://imaging.cs.cmu.edu/dual_pixels/">
                <papertitle>Defocus Map Estimation and Deblurring from a Single Dual-Pixel Image</papertitle>
              </a>
              <br>
              <a href="https://shumianxin.github.io/">Shumian Xin</a>,
              <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
              <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
              <strong>Jonathan T. Barron</strong>, <br>
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
              <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
							<a href="https://www.cs.cmu.edu/~igkioule/">Ioannis Gkioulekas</a>,
              <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>
              <br>
							<em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
							<br>
              <a href="https://imaging.cs.cmu.edu/dual_pixels/">project page</a> /
              <a href="https://github.com/cmu-ci-lab/dual_pixel_defocus_estimation_deblurring">code</a>
              <br>
              <p></p>
              <p>
                Multiplane images can be used to simultaneously deblur dual-pixel images, despite variable defocus due to depth variation in the scene.
              </p>
            </td>
          </tr> 


          <tr onmouseout="nerd_stop()" onmouseover="nerd_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerd_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/nerd_160.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerd_160.jpg' width="160">
              </div>
              <script type="text/javascript">
                function nerd_start() {
                  document.getElementById('nerd_image').style.opacity = "1";
                }

                function nerd_stop() {
                  document.getElementById('nerd_image').style.opacity = "0";
                }
                nerd_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://markboss.me/publication/2021-nerd/">
                <papertitle>NeRD: Neural Reflectance Decomposition from Image Collections</papertitle>
              </a>
              <br>

              <a href="https://markboss.me">Mark Boss</a>, 
              <a href="https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/raphael-braun/">Raphael Braun</a>,
              <a href="https://varunjampani.github.io">Varun Jampani</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="http://people.csail.mit.edu/celiu/">Ce Liu</a>,
              <a href="https://uni-tuebingen.de/en/faculties/faculty-of-science/departments/computer-science/lehrstuehle/computergrafik/computer-graphics/staff/prof-dr-ing-hendrik-lensch/">Hendrik P. A. Lensch</a>
              <br>
							<em>ICCV</em>, 2021
              <br>
              <a href="https://markboss.me/publication/2021-nerd/">project page</a> /
              <a href="https://www.youtube.com/watch?v=JL-qMTXw9VU">video</a> /
              <a href="https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition">code</a> /
              <a href="https://arxiv.org/abs/2012.03918">arXiv</a>
              <p></p>
              <p>
              A NeRF-like model that can decompose (and mesh) objects with non-Lambertian reflectances, complex geometry, and unknown illumination.
              </p>
            </td>
          </tr>

          <tr onmouseout="flare_stop()" onmouseover="flare_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='flare_image'>
                  <img src='images/flare_after.jpg' width="160"></div>
                <img src='images/flare_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function flare_start() {
                  document.getElementById('flare_image').style.opacity = "1";
                }

                function flare_stop() {
                  document.getElementById('flare_image').style.opacity = "0";
                }
                flare_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2011.12485">
                <papertitle>How to Train Neural Networks for Flare Removal</papertitle>
              </a>
              <br>
              <a href="http://yicheng.rice.edu/">Yicheng Wu</a>,
              <a href="https://scholar.google.com/citations?user=BxqV_RsAAAAJ">Qiurui He</a>,
              <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
              <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>, <br>
              <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
              <a href="https://computationalimaging.rice.edu/team/ashok-veeraraghavan/">Ashok Veeraraghavan</a>,
              <strong>Jonathan T. Barron</strong>
              <br>
							<em>ICCV</em>, 2021
              <br>
              <a href="https://yichengwu.github.io/flare-removal/">project page</a>  / 
              <a href="https://arxiv.org/abs/2011.12485">arXiv</a> 
              <p></p>
              <p>
                Simulating the optics of a camera's lens lets you train a model that removes lens flare from a single image.
              </p>
            </td>
          </tr> 


          <tr onmouseout="inerf_stop()" onmouseover="inerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='inerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/inerf_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/inerf_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function inerf_start() {
                  document.getElementById('inerf_image').style.opacity = "1";
                }
                function inerf_stop() {
                  document.getElementById('inerf_image').style.opacity = "0";
                }
                inerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://yenchenlin.me/inerf/">
                <papertitle>iNeRF: Inverting Neural Radiance Fields for Pose Estimation</papertitle>
              </a>
              <br>
              <a href="https://yenchenlin.me/">Lin Yen-Chen</a>, 
              <a href="http://www.peteflorence.com/">Pete Florence</a>, 
              <strong>Jonathan T. Barron</strong>,  <br>
              <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,
              <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>,
              <a href="https://scholar.google.com/citations?user=_BPdgV0AAAAJ&hl=en">Tsung-Yi Lin</a>
              <br>
              <em>IROS</em>, 2021  
              <br>
              <a href="http://yenchenlin.me/inerf/">project page</a> /
              <a href="https://arxiv.org/abs/2012.05877">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=eQuCZaQN0tI">video</a>
              <p></p>
              <p>Given an image of an object and a NeRF of that object, you can estimate that object's pose.
              </p>
            </td>
          </tr> 

          <tr onmouseout="ibrnet_stop()" onmouseover="ibrnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ibrnet_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/ibrnet_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/ibrnet_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function ibrnet_start() {
                  document.getElementById('ibrnet_image').style.opacity = "1";
                }

                function ibrnet_stop() {
                  document.getElementById('ibrnet_image').style.opacity = "0";
                }
                ibrnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ibrnet.github.io/">
                <papertitle>IBRNet: Learning Multi-View Image-Based Rendering</papertitle>
              </a>
              <br>
              <a href="https://www.cs.cornell.edu/~qqw/">Qianqian Wang</a>,
              <a href="https://www.linkedin.com/in/zhicheng-wang-96116897/">Zhicheng Wang</a>,
              <a href="https://www.kylegenova.com/">Kyle Genova</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
              <a href="https://scholar.google.com/citations?user=Rh9T3EcAAAAJ&hl=en">Howard Zhou</a>, <br>
              <strong>Jonathan T. Barron</strong>, 
              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
              <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>, 
              <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://ibrnet.github.io/">project page</a> /
              <a href="https://github.com/googleinterns/IBRNet">code</a> / 
              <a href="https://arxiv.org/abs/2102.13090">arXiv</a>
              <p></p>
              <p>By learning how to pay attention to input images at render time, 
                  we can amortize inference for view synthesis and reduce error rates by 15%.</p>
            </td>
          </tr>

          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerv_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/hotdog.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/hotdog.jpg' width="160">
              </div>
              <script type="text/javascript">
                function nerv_start() {
                  document.getElementById('nerv_image').style.opacity = "1";
                }

                function nerv_stop() {
                  document.getElementById('nerv_image').style.opacity = "0";
                }
                nerv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://pratulsrinivasan.github.io/nerv/">
                <papertitle>NeRV: Neural Reflection and Visibility Fields for Relighting and View Synthesis</papertitle>
              </a>
              <br>
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
              <a href="https://boyangdeng.com/">Boyang Deng</a>,
              <a href="https://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>, <br>
              <a href="http://matthewtancik.com/">Matthew Tancik</a>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <strong>Jonathan T. Barron</strong>
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://pratulsrinivasan.github.io/nerv/">project page</a> /
              <a href="https://www.youtube.com/watch?v=4XyDdvhhjVo">video</a> /
              <a href="https://arxiv.org/abs/2012.03927">arXiv</a>
              <p></p>
              <p>Using neural approximations of expensive visibility integrals lets you recover relightable NeRF-like models.</p>
            </td>
          </tr>


          <tr onmouseout="winr_stop()" onmouseover="winr_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='winr_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/notre_160.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/notre.jpg' width="160">
              </div>
              <script type="text/javascript">
                function winr_start() {
                  document.getElementById('winr_image').style.opacity = "1";
                }
                function winr_stop() {
                  document.getElementById('winr_image').style.opacity = "0";
                }
                winr_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://www.matthewtancik.com/learnit">
                <papertitle>Learned Initializations for Optimizing Coordinate-Based Neural Representations</papertitle>
              </a>
              <br>
              <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
              <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
              <a href="https://www.linkedin.com/in/terrance-wang/">Terrance Wang</a>,
              <a href="https://www.linkedin.com/in/divi-schmidt-262044180/">Divi Schmidt</a>, <br>
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
              <br>
              <em>CVPR</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="http://www.matthewtancik.com/learnit">project page</a> /
              <a href="https://www.youtube.com/watch?v=A-r9itCzcyo">video</a> /
              <a href="https://arxiv.org/abs/2012.02189">arXiv</a> 
              <p></p>
              <p>Using meta-learning to find weight initializations for coordinate-based MLPs allows them to converge faster and generalize better.</p>
            </td>
          </tr>

          <tr onmouseout="nerfw_stop()" onmouseover="nerfw_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfw_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/nerfw_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerfw_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function nerfw_start() {
                  document.getElementById('nerfw_image').style.opacity = "1";
                }

                function nerfw_stop() {
                  document.getElementById('nerfw_image').style.opacity = "0";
                }
                nerfw_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://nerf-w.github.io/">
                <papertitle>NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections</papertitle>
              </a>
              <br>
              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla*</a>,
              <a href="https://scholar.google.com/citations?user=g98QcZUAAAAJ&hl=en">Noha Radwan*</a>,
              <a href="https://research.google/people/105804/">Mehdi S. M. Sajjadi*</a>, <br>
              <strong>Jonathan T. Barron</strong>,
              <a href="https://scholar.google.com/citations?user=FXNJRDoAAAAJ&hl=en">Alexey Dosovitskiy</a>,
              <a href="http://www.stronglyconvex.com/about.html">Daniel Duckworth</a>
              <br>
              <em>CVPR</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://nerf-w.github.io/">project page</a> /
              <a href="https://arxiv.org/abs/2008.02268">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=mRAKVQj5LRA">video</a>
              <p></p>
              <p>Letting NeRF reason about occluders and appearance variation produces photorealistic view synthesis using only unstructured internet photos.</p>
            </td>
          </tr> 

          <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dualrefl_image'>
                  <img src='images/dualrefl_after.jpg' width="160"></div>
                <img src='images/dualrefl_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function dualrefl_start() {
                  document.getElementById('dualrefl_image').style.opacity = "1";
                }

                function dualrefl_stop() {
                  document.getElementById('dualrefl_image').style.opacity = "0";
                }
                dualrefl_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://sniklaus.com/dualref">
                <papertitle>Learned Dual-View Reflection Removal</papertitle>
              </a>
              <br>
              <a href="http://sniklaus.com/welcome">Simon Niklaus</a>,
              <a href="https://people.eecs.berkeley.edu/~cecilia77/">Xuaner (Cecilia) Zhang</a>,
              <strong>Jonathan T. Barron</strong>, <br>
              <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
              <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
              <a href="http://web.cecs.pdx.edu/~fliu/">Feng Liu</a>,
              <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>
              <br>
              <em>WACV</em>, 2021
              <br>
              <a href="http://sniklaus.com/dualref">project page</a> /
              <a href="https://arxiv.org/abs/2010.00702">arXiv</a>
              <p></p>
              <p>
                Reflections and the things behind them often exhibit parallax, and this lets you remove reflections from stereo pairs.
              </p>
            </td>
          </tr> 


          <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nlt_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/nlt_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nlt_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function nlt_start() {
                  document.getElementById('nlt_image').style.opacity = "1";
                }

                function nlt_stop() {
                  document.getElementById('nlt_image').style.opacity = "0";
                }
                nlt_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://nlt.csail.mit.edu/">
                <papertitle>Neural Light Transport for Relighting and View Synthesis</papertitle>
              </a>
              <br>
              <a href="http://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
              <a href="http://www.seanfanello.it/">Sean Fanello</a>,
              <a href="https://research.google/people/105312/">Yun-Ta Tsai</a>,
              <a href="http://kevinkingo.com/">Tiancheng Sun</a>,
              <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
              <a href="https://research.google/people/106687/">Rohit Pandey</a>,
              <a href="https://www.dtic.ua.es/~sorts/">Sergio Orts-Escolano</a>,
              <a href="https://dl.acm.org/profile/99659224296">Philip Davidson</a>,
              <a href="https://scholar.google.com/citations?user=5D0_pjcAAAAJ&hl=en">Christoph Rhemann</a>,
              <a href="http://www.pauldebevec.com/">Paul Debevec</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="http://billf.mit.edu/">William T. Freeman</a>
              <br>
              <em>ACM TOG</em>, 2021
              <br>
              <a href="http://nlt.csail.mit.edu/">project page</a> /
              <a href="https://arxiv.org/abs/2008.03806">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=OGEnCWZihHE">video</a>
              <p></p>
              <p>Embedding a convnet within a predefined texture atlas enables simultaneous view synthesis and relighting.</p>
            </td>
          </tr> 

          <tr onmouseout="lssr_stop()" onmouseover="lssr_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='lssr_image'>
                  <img src='images/lssr_after.jpg' width="160"></div>
                <img src='images/lssr_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function lssr_start() {
                  document.getElementById('lssr_image').style.opacity = "1";
                }

                function lssr_stop() {
                  document.getElementById('lssr_image').style.opacity = "0";
                }
                lssr_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://cseweb.ucsd.edu/~viscomp/projects/SIGA20LightstageSuperres/">
                <papertitle>Light Stage Super-Resolution: Continuous High-Frequency Relighting</papertitle>
              </a>
              <br>
              <a href="http://kevinkingo.com/">Tiancheng Sun</a>,
              <a href="https://cseweb.ucsd.edu/~zex014/">Zexiang Xu</a>
              <a href="http://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
              <a href="http://www.seanfanello.it/">Sean Fanello</a>,
              <a href="https://scholar.google.com/citations?user=5D0_pjcAAAAJ&hl=en">Christoph Rhemann</a>, <br>
              <a href="https://www.pauldebevec.com/">Paul Debevec</a>,
              <a href="https://research.google/people/105312/">Yun-Ta Tsai</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
              <br>
              <em>SIGGRAPH Asia</em>, 2020  
              <br>
              <a href="http://cseweb.ucsd.edu/~viscomp/projects/SIGA20LightstageSuperres/">project page</a> / 
              <a href="https://arxiv.org/abs/2010.08888">arXiv</a>
              <p></p>
              <p>
                Scans for light stages are inherently aliased, but we can use learning to super-resolve them.
              </p>
            </td>
          </tr> 


          <tr onmouseout="ff_stop()" onmouseover="ff_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/lion_ff.jpg' width="160"></div>
                <img src='images/lion_none.jpg' width="160">
              </div>
              <script type="text/javascript">
                function ff_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function ff_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                ff_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://bmild.github.io/fourfeat/index.html">
                <papertitle>Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains</papertitle>
              </a>
              <br>
              <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan*</a>,
              <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
              <a href="https://people.eecs.berkeley.edu/~sfk/">Sara Fridovich-Keil</a>, <br>
              <a href="https://www.linkedin.com/in/nithinraghavan">Nithin Raghavan</a>,
              <a href="https://scholar.google.com/citations?user=lvA86MYAAAAJ&hl=en">Utkarsh Singhal</a>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
              <br>
              <em>NeurIPS</em>, 2020 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
              <br>
              <a href="https://bmild.github.io/fourfeat/">project page</a> /
              video: <a href="https://www.youtube.com/watch?v=nVA6K6Sn2S4">3 min</a>, <a href="https://www.youtube.com/watch?v=iKyIJ_EtSkw">10 min</a> /
              <a href="https://arxiv.org/abs/2006.10739">arXiv</a> /
              <a href="https://github.com/tancik/fourier-feature-networks">code</a>
              <p></p>
              <p>Composing neural networks with a simple Fourier feature mapping allows them to learn detailed high-frequency functions.</p>
            </td>
          </tr> 


        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
              <br>
              <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a>
              <br>
              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
              <br>
              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cs188.jpg" alt="cs188">
            </td>
            <td width="75%" valign="center">
              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
              <br>
              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
              <br>
              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
            </td>
          </tr>
					

          <tr>
            <td align="center" style="padding:20px;width:25%;vertical-align:middle">
							<heading>Basically <br> Blog Posts</heading>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
              <br>
              <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
              <br>
              <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
            </td>
          </tr>
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>,
                just add a link back to my website.
                <strong>Do not</strong> scrape the HTML from the deployed instance of this website at http://jonbarron.info,
                as it includes analytics tags that you do not want on your own website &mdash; use the github code instead.
                If you'd like your new page linked to from here, submit a pull request adding yourself.
                <a href="https://vjysd.github.io/">&#10025;</a>
                <a href="https://www.cs.ubc.ca/~wsgh/">&#10025;</a>
                <br>
                Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
